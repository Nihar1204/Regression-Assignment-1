{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028d967a-7817-4f9d-b7c2-074cdb3dfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "\n",
    "# Simple Linear Regression:\n",
    "# This is a statistical method used to establish a relationship between two variables: an independent variable (predictor) and\n",
    "# a dependent variable (outcome). The relationship is represented by a straight line equation: y=mx+c, where 'y' is the \n",
    "# dependent variable, 'x' is the independent variable, 'm' is the slope of the line, and 'c' is the y-intercept. It aims to\n",
    "# predict the value of the dependent variable based on the value of a single independent variable.\n",
    "\n",
    "# Example:\n",
    "# Let's say we want to predict student's test score (dependent variable) based on the number of hours they \n",
    "# studied (independent variable). \n",
    "\n",
    "# Multiple Linear Regression:\n",
    "# This extends the concept of simple linear regression by involving more than one independent variable to predict a dependent\n",
    "# variable. Instead of a single predictor, it uses multiple predictors to estimate the outcome variable.\n",
    "\n",
    "# Example:\n",
    "# Predicting price of the house with independent features like size of the rooms, number of rooms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1f5bb4-a510-4378-bca8-7e2e6f445c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "\n",
    "# Linearity: The relationship between the independent and dependent variables should be linear. We can assess this by creating\n",
    "# scatter plots of variables against the response variable. If the points follow a roughly linear pattern, the assumption \n",
    "# might hold.\n",
    "\n",
    "# No Perfect Multicollinearity: In multiple linear regression, the independent variables should not be perfectly correlated\n",
    "# with each other. High correlation between predictors might cause issues in estimating the coefficients properly. Variance\n",
    "# Inflation Factor (VIF) can be used to detect multicollinearity.\n",
    "\n",
    "# Normality of Residuals: The residuals should ideally follow a normal distribution. We can use a histogram or a Q-Q plot to \n",
    "# visualize if the residuals are approximately normally distributed. If the residuals deviate significantly from normality, \n",
    "# this assumption might not hold.\n",
    "\n",
    "# Independence of Errors: The residuals should be independent of each other. This means that one residual should not predict \n",
    "# or influence the next residual. A residual plot over time (if applicable) or against the order of observations should \n",
    "# ideally show no pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66f6c63-00b2-4711-9a3e-21607217aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "\n",
    "# Slope(m): Slope is the unit change in between the Y-axis over the X-axis. In other words it indicates the rate of change in\n",
    "# the dependent variable concerning the independent variable.\n",
    "\n",
    "# Intercept(c): The intercept is the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "# Example: Predicting Salary Based on Years of Experience\n",
    "# Suppose we're analyzing the relationship between years of experience and salary. We collect data from a company where we\n",
    "# find that employees with varying years of experience have different salaries.\n",
    "# Our linear regression model might look like: Salary = slope * years of Experience + intercept\n",
    "# Let's assume here slope is 5000 and intercept is 25000 so we can easily get the Salary by putting years of Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58ebc55-a6fd-44e1-b805-051809d32110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "\n",
    "# Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. In machine \n",
    "# learning, we often aim to minimize a cost or loss function that measures the disparity between predicted and actual values.\n",
    "# The goal is to find the model parameters that minimize this function.\n",
    "\n",
    "# Gradient Descent Steps:\n",
    "# Initialization: It starts with initializing the model's parameters with some arbitrary values.\n",
    "# Iterative Updates: The algorithm iteratively updates these parameters to minimize the loss function by moving in the \n",
    "# direction of steepest descent (negative gradient direction).\n",
    "# Gradient Calculation: It computes the gradient of the loss function with respect to each parameter. The gradient gives the\n",
    "# direction and magnitude of the steepest increase of the function.\n",
    "#  Parameter Update: Using the gradient information, it adjusts the parameters by taking steps proportional to the negative \n",
    "# of the gradient, scaled by a learning rate. This learning rate determines how big the steps are in each iteration.\n",
    "# Convergence: The process continues iteratively, adjusting the parameters until the algorithm converges to a minimum \n",
    "# of the loss function or reaches a predefined stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbdb5969-4ece-4419-be87-a41234e44db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "\n",
    "# Multiple linear regression is an extension of simple linear regression that involves more than one independent\n",
    "# variable in predicting a dependent variable.\n",
    "\n",
    "# Differences from Simple Linear Regression:\n",
    "# Number of Predictors: Simple linear regression involves only one independent variable, whereas multiple linear regression\n",
    "# includes multiple independent variables.\n",
    "# Equation Complexity: The equation for multiple linear regression includes multiple predictors and coefficients, while simple\n",
    "# linear regression has a simpler equation with only one predictor and coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a248d04-ce5c-4457-8df8-f34ca90ab61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6\n",
    "\n",
    "# Multicollinearity: Multicollinearity in multiple linear regression refers to a situation where two or more predictor\n",
    "# variables in the model are highly correlated, leading to issues in the estimation of coefficients and\n",
    "# interpretation of the model.\n",
    "\n",
    "# Issues caused by multicollinearity:\n",
    "# Unreliable Coefficients: When predictor variables are highly correlated, it becomes challenging for the model to \n",
    "# differentiate the individual effect of each variable on the dependent variable. This can lead to coefficients that don't \n",
    "# make sense or have unexpected signs.\n",
    "# Reduced Precision: Multicollinearity inflates the standard errors of the coefficients, making them less precise. As a result,\n",
    "# confidence intervals become wider, affecting the statistical significance of predictors.\n",
    "\n",
    "# Detection of Multicollinearity:\n",
    "# Correlation Matrix: Calculate the correlation matrix among the predictors. High correlation coefficients (close to 1 or -1)\n",
    "# indicate potential multicollinearity.\n",
    "# Variance Inflation Factor (VIF): VIF measures how much the variance of a coefficient is inflated due to multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ce3f31-3ded-4087-8d91-db0f10e02764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "\n",
    "# Polynomial regression is a type of regression analysis that allows for a nonlinear relationship between the independent \n",
    "# variables and the dependent variable by using higher-degree polynomial functions.\n",
    "\n",
    "# Difference:\n",
    "# While linear regression aims to model the relationship between the dependent variable and one or more independent variables\n",
    "# as a linear equation (straight line or plane in higher dimensions), polynomial regression models this relationship using\n",
    "# a polynomial equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b725695-3a9a-4b4a-9437-a21cf72f16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "\n",
    "# Advantages of Polynomial Regression:\n",
    "# Flexibility: Polynomial regression can fit nonlinear relationships between variables, capturing more complex patterns \n",
    "# that linear regression cannot.\n",
    "# Better Fit: It can provide a better fit to the data, especially when the true relationship between variables is nonlinear.\n",
    "# Higher Order Relationships: Allows modeling of higher order relationships between variables, enabling the capture of \n",
    "# curvature, bends, and various shapes in the data.\n",
    "\n",
    "# Disadvantages of Polynomial Regression:\n",
    "# Overfitting Risk: Higher-degree polynomials can lead to overfitting, capturing noise or random fluctuations in the \n",
    "# training data and reducing model generalization to new data.\n",
    "# Complexity: Higher-degree polynomials increase model complexity, making interpretation of coefficients more challenging \n",
    "# and potentially leading to a less interpretable model.\n",
    "# Extrapolation Issues: Extrapolating beyond the range of observed data with higher-degree polynomials can result \n",
    "# in unreliable predictions.\n",
    "\n",
    "# Situations Favoring Polynomial Regression:\n",
    "# Nonlinear Relationships: When the relationship between variables is clearly nonlinear, polynomial regression is a better\n",
    "# choice to capture the complexity of this relationship.\n",
    "# Complex Patterns: In scenarios where the data exhibits curves, bends, or patterns that a straight line cannot capture\n",
    "# adequately, polynomial regression can be beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c885c-fd6b-45fa-bca2-b9656308f9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
